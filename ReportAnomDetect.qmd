---
title: "Time series anomaly detection - Canadian weather data"
author: "Ahmet Zamanis"
format:
  html: 
    toc: true
editor: visual
jupyter: python3
execute:
  warning: false
---

## Introduction

This report will apply & compare several anomaly detection algorithms on a multivariate time series dataset of weather measurements in Canada. We'll use numerous algorithms from the [PyOD](https://github.com/yzhao062/pyod) anomaly detection package, along with time series data formats & wrappers from the anomaly detection module of the [Darts](https://github.com/unit8co/darts) package. We'll also implement a simple autoencoder using PyTorch Lightning. We'll use numerous visualizations, including 3D Plotly scatterplots to illustrate the results and unique outputs of each algorithm.

The data consists of daily mean temperature and total precipitation measurements across 13 Canadian locations, some ranging from 1940 to 2020, others starting from 1960. The data was downloaded from [OpenML](https://openml.org/search?type=data&status=active&id=43843&sort=runs), shared by user Elif Ceren GÃ¶k.

The Python scripts for this analysis are available on [GitHub](https://github.com/AhmetZamanis/WeatherAnomalyDetectionClassification). They may not be fully up to date with the report.

```{python Imports}
#| code-fold: true
#| code-summary: "Show imports"

# Data handling
import pandas as pd
import numpy as np
import arff # Installed as liac-arff
import warnings

# Plotting
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# Preprocessing
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from darts.dataprocessing.transformers.scaler import Scaler
from darts.dataprocessing.transformers.missing_values_filler import MissingValuesFiller

# Darts
from darts.timeseries import TimeSeries
from darts.ad.scorers.kmeans_scorer import KMeansScorer
from darts.ad.scorers.pyod_scorer import PyODScorer
from darts.ad.detectors.quantile_detector import QuantileDetector

# PyOD anomaly scorers
from pyod.models.gmm import GMM
from pyod.models.ecod import ECOD
from pyod.models.pca import PCA
from pyod.models.iforest import IForest

# Torch and Lightning
import torch
import lightning as L
from X_LightningClassesAnom import TrainDataset, TestDataset, AutoEncoder

# Hyperparameter tuning
import optuna

# Dimensionality reduction
from sklearn.manifold import TSNE

# Helper functions
from X_HelperFunctionsAnom import score, detect, plot_series, plot_dist, plot_anom3d, plot_detection, validate_nn

```

```{python Settings}
#| code-fold: true
#| code-summary: "Show settings"

# Set print options
np.set_printoptions(suppress=True, precision=4)
pd.options.display.float_format = '{:.4f}'.format
pd.set_option('display.max_columns', None)

# Set plotting options
plt.rcParams['figure.dpi'] = 150
plt.rcParams["figure.autolayout"] = True
sns.set_style("darkgrid")
px_width = 800
px_height = 800

# Set Torch settings
torch.set_default_dtype(torch.float32)
torch.set_float32_matmul_precision('high')
L.seed_everything(1923, workers = True)
warnings.filterwarnings("ignore", ".*does not have many workers.*")

```

## Data preparation

The data is in .arff format, and we'll use the [liac-arff](https://github.com/renatopp/liac-arff) package to read it. We have daily mean temperature and total precipitation values for each of the 13 locations.

```{python LoadData}

# Load raw data from .arff
raw_data = arff.load(open("./InputData/canadian_climate.arff", "r"))

# Convert to Pandas dataframe & view
df = pd.DataFrame(
  raw_data["data"], columns = [x[0] for x in raw_data["attributes"]])
print(df.iloc[:,0:4])

```

To score anomalies, we need to compare the weather variables with the time variables: The same weather measurements can be perfectly normal or anomalous depending on the season. We are looking for **local anomalies,** not global.

-   To this end, we'll extract month, week of year and day of year features from our date column.

-   We'll use cyclical encoding to transform these categorical variables into numeric, while reflecting their cyclical nature to the algorithms we'll use. This creates a pair of sine and cosine values for each time variable, resulting in 8 variables per location.

```{python TimeFeatures}

# Convert LOCAL_DATE to datetime
df["LOCAL_DATE"] = pd.to_datetime(df["LOCAL_DATE"])

# Add cyclic terms for month, week of year and day of year
df["month_sin"] = np.sin(2 * np.pi * df["LOCAL_DATE"].dt.month / 12)
df["month_cos"] = np.cos(2 * np.pi * df["LOCAL_DATE"].dt.month / 12)
df["week_sin"] = np.sin(2 * np.pi * df["LOCAL_DATE"].dt.isocalendar().week / 53)
df["week_cos"] = np.cos(2 * np.pi * df["LOCAL_DATE"].dt.isocalendar().week / 53)
df["day_sin"] = np.sin(2 * np.pi * df["LOCAL_DATE"].dt.dayofyear / 366)
df["day_cos"] = np.cos(2 * np.pi * df["LOCAL_DATE"].dt.dayofyear / 366)

```

We'll focus our attention only on Ottawa, one of the locations that has data available from 1940. There are a few missing weather measurements across the time range, and we'll interpolate them.

```{python TSOttawa}

# Retrieve the weather data for Ottawa (1940 start) as Darts TimeSeries
ts_ottawa = TimeSeries.from_dataframe(
  df,
  time_col = "LOCAL_DATE",
  value_cols = ["MEAN_TEMPERATURE_OTTAWA", "TOTAL_PRECIPITATION_OTTAWA"],
  fill_missing_dates = True
)

# Interpolate missing values
na_filler = MissingValuesFiller()
ts_ottawa = na_filler.transform(ts_ottawa)

```

We don't have any missing values for our time variables, as we created them from the dates.

```{python TSCovars}

# Retrieve date covariates as Darts TS
ts_covars = TimeSeries.from_dataframe(
  df,
  time_col = "LOCAL_DATE",
  value_cols = ['month_sin', 'month_cos', 'week_sin', 'week_cos', 'day_sin', 
  'day_cos'],
  fill_missing_dates = True
)

```

We'll split our data in roughly two: The anomaly scorers will be trained on data before 1980, and "tested" on data after 1980, though we won't have any objective performance metrics, as we don't have true anomaly labels to compare our models' output with.

```{python Preprocess}

# Concatenate time covariates to Ottawa weather series
ts_ottawa = ts_ottawa.concatenate(ts_covars, axis = 1)

# Split train-test data
test_start = pd.Timestamp("1980-01-01")
train_end = pd.Timestamp("1979-12-31")
ts_train = ts_ottawa.drop_after(test_start)
ts_test = ts_ottawa.drop_before(train_end)

```

## Anomaly detection with PyOD & Darts

Most of the algorithms we'll use are sensitive to features with differing value scales, so we'll scale our features from -1 to 1. This is the value range for the cyclical encoded columns, so they will be unchanged.

We'll also create a Darts anomaly detector, with a high quantile threshold. It will take in the anomaly scores produced by the scorers, and flag the top Qth quantile as anomalies. It's also possible to use a raw score threshold detector.

```{python Utilities}

# Create Darts wrapper for MinMaxScaler
feature_range = (-1, 1) # The value range of cyclical encoded features
scaler = Scaler(MinMaxScaler(feature_range = feature_range))

# Create quantile detector
q = 0.999 # The detector will flag scores above this quantile as anomalies
detector = QuantileDetector(high_quantile = q)

```

To shorten the code, we'll use several functions to concisely train our models, anomaly score the data & create plots. These are available [here](https://github.com/AhmetZamanis/WeatherAnomalyDetectionClassification/blob/main/X_HelperFunctionsAnom.py).

### K-means clustering

Our first anomaly scorer is based on the K-means clustering algorithm. K number of clusters are fit to the data, and anomaly scores for each observation are calculated as the smallest distance from the nearest cluster.

-   As an intuitive choice for the number of clusters, we'll go with 12, one for each month of the year. Another decent choice is likely 4, one for each season.

The `window` parameter controls the number of observations that will be anomaly scored together. Since we're interested in daily weather anomalies, we'll use the default value of 1. For many time series anomaly detection tasks, larger windows will make more sense.

-   For example, when anomaly scoring data from financial markets, a single unusual day may not be very meaningful, but a whole week of trading with unusual values may be an important signal.

-   In such a case, with `window = 7`, Darts reformats the data so each row is a sequence of 7 days. Anomaly scoring is performed on each sequence instead of single observations.

We fit the anomaly scorer on the training period, and generate anomaly scores both for the training & testing periods.

```{python KMeansScore}

# Create K-means scorer
scorer_kmeans = KMeansScorer(
  window = 1, # Score each time step by itself
  k = 12, # Number of K-means clusters
  random_state = 1923)

# Perform anomaly scoring
scores_train_kmeans, scores_test_kmeans, scores_kmeans = score(
  ts_train, ts_test, scorer_kmeans, scaler)

```

We pass the anomaly scores to our detector, and get binary anomaly labels.

```{python KMeansDetect}

# Perform anomaly detection
anoms_train_kmeans, anoms_test_kmeans, anoms_kmeans = detect(
  scores_train_kmeans, scores_test_kmeans, detector)

```

Since we don't have true anomaly labels, there aren't any performance metrics we can calculate. Instead, we'll plot the anomaly scores along with the weather measurements.

```{python KMeansPlotSeries}

# Plot anomaly scores & original series
plot_series(
  "K-means scorer", ts_train, ts_test, scores_train_kmeans, scores_test_kmeans)
  
```

From the plots above, it appears the K-means anomaly scores are highly related to the total precipitation values. A particular post-2000 day with very high precipitation stands out.

-   The overall anomaly scores and weather measurements don't seem too different before and after 1980, though a few post-1980 days particularly stand out.

Next, we'll plot the same variables, but this time we'll color by anomaly labels. The black line will be the observations with bottom Qth quantile of anomaly scores, and the blue line will be the top Qth, the flagged anomalies.

```{python KMeansPlotDetect}

# Detections plot
plot_detection("K-means scores", q, ts_ottawa, scores_kmeans, anoms_kmeans)

```

We see the anomaly scores for the flagged observations are clearly and considerably higher than non-flagged ones, which is good. Ideally we want as much separation between the anomaly scores as possible to avoid false positives or false negatives.

-   The anomalies all have the highest observed precipitation values. Their temperature values are not unusual compared to the remaining observations, but they are generally on the higher side.

-   In summary, the K-means scorer identifies warm days with unusually high precipitation as anomalies.

Next, we'll look at the anomaly score density plots for the training and testing periods, comparing their distributions.

```{python KMeansPlotDist}

# Plot distributions of anomaly scores
plot_dist("K-means scorer", scores_train_kmeans, scores_test_kmeans)

```

The K-means anomaly scores are distributed similarly, though the post-1980 period does have a bit more of the higher-scored observations, and a bit less of the average-scored observations.

Next, we'll plot all observations in a 3D scatterplot, with our weather variables and the month as our three dimensions. We'll color the observations by their anomaly labels. The interactive plot can be moved around by clicking and holding.

```{python KMeansPlot3D}

# 3D anomalies plot
plot_anom3d(
  "K-means scorer", ts_ottawa, anoms_kmeans, px_width, px_height, html = True)

```

Here we see the vast majority of anomalies are summer months with very high precipitation, though not unusual temperatures. Some anomalies also belong to the fall months, and a single one is in May.

We can also retrieve the fitted cluster labels from the K-means algorithm, and use them to color the observations in the same 3D scatterplot.

```{python KMeansClustering}
#| code-fold: true
#| code-summary: "Show code for 3D clustering plot"

# Clustering plot
train_labels = scorer_kmeans.model.labels_.astype(str)
fig = px.scatter_3d(
  x = ts_train['MEAN_TEMPERATURE_OTTAWA'].univariate_values(),
  y = ts_train['TOTAL_PRECIPITATION_OTTAWA'].univariate_values(),
  z = ts_train.time_index.month,
  color = train_labels,
  title = "K-Means clustering plot, train set",
  labels = {
    "x": "Mean temperature",
    "y": "Total precipitation",
    "z": "Month",
    "color": "Clusters"},
    width = px_width,
    height = px_height
)
fig.show()

```

We see the K-means algorithm very clearly clusters the observations according to their months. The anomaly scores are then calculated as the distance from the closest cluster centroid, so the most anomalous observations are the high-precipitation ones in summer.

-   Compared to the summer months, we don't see such particularly distant observations in winter.

### Gaussian mixture models (GMM)

The GMM approach assumes that the data distribution consists of the combination of an N number of Gaussian distributions. These N distributions are learned from the fitted data. Anomaly scores are computed for observations based on their likelihoods.

-   In practice, this approach is fairly similar to K-means clustering. Just like the choice of K clusters in K-means, we'll choose the number of Gaussian mixture components as a parameter.

    -   We'll go with `n_components = 4`, one for each season, just for variety against the previous approach of 12 cluster centroids.

    -   GMM is trained with expectation maximization, so the solution is not deterministic, though convergence to a local solution is guaranteed. We'll perform 10 initializations, and the best results will be kept.

Darts offers the `PyODScorer` time series wrapper for any algorithm in `PyOD`.

```{python CreateGMM}

# Create PyOD GMM model
gmm = GMM(
  n_components = 4, # N. of Gaussian mixture components
  n_init = 10, # N. of initializations for expectation maximization
  contamination = (1 - q), # Proportion of expected anomalies in the dataset
  random_state = 1923)
  
# Create Darts GMM scorer  
scorer_gmm = PyODScorer(model = gmm, window = 1)

```

We won't repeat the entire anomaly detection & plotting code for each algorithm, but just illustrate the differences and comment on the outputs.

```{python GMMCode}
#| code-fold: true
#| code-summary: "Show code for GMM anomaly detection"

# Perform anomaly scoring
scores_train_gmm, scores_test_gmm, scores_gmm = score(
  ts_train, ts_test, scorer_gmm, scaler)

# Perform anomaly detection
anoms_train_gmm, anoms_test_gmm, anoms_gmm = detect(
  scores_train_gmm, scores_test_gmm, detector)

# Plot scores & original series
plot_series("GMM scorer", ts_train, ts_test, scores_train_gmm, scores_test_gmm)

# Detections plot
plot_detection("GMM scores", q, ts_ottawa, scores_gmm, anoms_gmm)

# Plot distributions of anomaly scores
plot_dist("GMM scorer", scores_train_gmm, scores_test_gmm)

# 3D anomalies plot
plot_anom3d("GMM scorer", ts_ottawa, anoms_gmm, px_width, px_height, html = True)

# 3D cluster plot
labels = scorer_gmm.model.detector_.predict(
  scaler.transform(ts_ottawa).values()).astype(str)
fig = px.scatter_3d(
  x = ts_ottawa['MEAN_TEMPERATURE_OTTAWA'].univariate_values(),
  y = ts_ottawa['TOTAL_PRECIPITATION_OTTAWA'].univariate_values(),
  z = ts_ottawa.time_index.month,
  color = labels,
  title = "GMM clustering plot",
  labels = {
    "x": "Mean temperature",
    "y": "Total precipitation",
    "z": "Month",
    "color": "Clusters"},
    width = px_width,
    height = px_height
)
fig.show()

```

The GMM algorithm's outputs are fairly similar to K-means overall.

-   The differences between scores for anomalous and non-anomalous observations appear to be even larger in magnitude, which is good. The particular anomaly after year 2000 stands out even more compared to other anomalies.

-   We could say the GMM anomalies are a bit colder & less rainy overall.

-   The score distributions of train & test periods are even more similar, and have very high kurtosis. While it may be easier to identify individual anomalies, the overall comparison of scores across periods may be less meaningful.

-   Unlike K-means, we now have a few anomalies in all months of the year, including winter months. This is likely because the scoring is now based on 4 mixture components (seasons) instead of 12 clusters (months).

    -   For example, it's possible the anomalies in February are flagged because they are compared with the same mixture component as observations from January. In contrast, they were likely just compared with the "January cluster" in K-means, and appeared relatively non-anomalous.

    -   If we were interested in seasonal anomalies, this approach could be preferable, and for monthly anomalies K-means could be preferable (or, either approach could be fit with either number of components / clusters).

-   The fitted mixture components correspond to each season, as can be seen in the 3D clustering plot.

### ECOD

ECOD is a fairly simple and quick anomaly detection method which relies on empirical cumulative distribution functions. Let's make a few definitions to understand how.

-   For a value $x$, drawn from a random variable $X$, the cumulative distribution function $f(x)$ is the cumulative sum of the probability density functions for values $<=x$. This also corresponds to the probability that $X <= x$.

-   The empirical CDF is a step function obtained from an empirical sample measurement.

ECOD estimates a non-parametric eCDF from each dimension (feature) in the training data.

-   Tail probabilities are also estimated for each dimension. For each observation, the tail probabilities are used to calculate a dimensional anomaly score. These are then aggregated.

-   ECOD is simple & efficient, with no parameters to choose. However, I believe the nature of the algorithm should not be able to capture the interactions in our multivariate time series (weather measurements dependent on the time features).

```{python CreateECOD}

# Create ECOD scorer
ecod = ECOD(contamination = (1 - q))
scorer_ecod = PyODScorer(model = ecod, window = 1)

```

```{python ECODCode}
#| code-fold: true
#| code-summary: "Show code for ECOD anomaly detection"

# Perform anomaly scoring
scores_train_ecod, scores_test_ecod, scores_ecod = score(
  ts_train, ts_test, scorer_ecod)

# Perform anomaly detection
anoms_train_ecod, anoms_test_ecod, anoms_ecod = detect(
  scores_train_ecod, scores_test_ecod, detector)

# Plot scores & original series
plot_series("ECOD scorer", ts_train, ts_test, scores_train_ecod, scores_test_ecod)

# Detections plot
plot_detection("ECOD scores", q, ts_ottawa, scores_ecod, anoms_ecod)

# Plot distributions of anomaly scores
plot_dist("ECOD scorer", scores_train_ecod, scores_test_ecod)

# 3D anomalies plot
plot_anom3d("ECOD scorer", ts_ottawa, anoms_ecod, px_width, px_height, html = True)

```

Indeed, we see the anomaly scores for the flagged anomalies are fairly close to the non-anomalous observations. The particularly high post-2000 observation is not even assigned a high score.

-   The flagged anomalies have temperature and precipitation values all over the value range.

-   The anomaly score distributions are considerably less right-skewed compared to previous algorithms, as ECOD does not attribute particularly high scores to any observation.

-   Looking at the 3D anomalies plot, ECOD still flags some of the rainy summer observations, but misses quite a few. There are also numerous flagged observations that seem fairly normal, especially in December.

-   

One intuitive feature of the PyOD ECOD implementation is the ability to explain each training point's anomaly scoring with a dimensional outlier score plot. We'll view the plot for the highest precipitation day in the training data, which was assigned the highest training anomaly score by the previous algorithms.

```{python ECODExplainer}

# Special to ECOD scorer: Explain a single training point's outlier scoring
idx_max_precip = np.argmax(
  ts_train['TOTAL_PRECIPITATION_OTTAWA'].univariate_values())
scorer_ecod.model.explain_outlier(
  ind = idx_max_precip, # Index of point to explain
  columns = [0, 1, 2, 3], # Dimensions to explain
  feature_names = ["MeanTemp", "TotalPrecip", "MonthSin", "MonthCos"])

```

We see the precipitation value for this observation is assigned a higher than 0.999th quantile anomaly score, while the temperature anomaly score is fairly low.

-   As stated before, I believe the ECOD algorithm is not suitable to this task, as I don't think it takes the interactions between weather and time features into account. The scores for each feature are simply aggregated.

-   Furthermore, the dimensional anomaly scores for the time features are meaningless and potentially misleading, as it's not possible to observe "anomalous" values for these. They are simply there for interaction with the weather measurements.

### Principal components analysis (PCA)

```{python CreatePCA}

# Create PCA scorer
pca = PCA(contamination = (1 - q), standardization = True, random_state = 1923)
scorer_pca = PyODScorer(model = pca, window = 1)

```

```{python PCACode}
#| code-fold: true
#| code-summary: "Show code for PCA anomaly detection"

# Perform anomaly scoring
scores_train_pca, scores_test_pca, scores_pca = score(
  ts_train, ts_test, scorer_pca)

# Perform anomaly detection
anoms_train_pca, anoms_test_pca, anoms_pca = detect(
  scores_train_pca, scores_test_pca, detector)

# Plot scores & original series
plot_series("PCA scorer", ts_train, ts_test, scores_train_pca, scores_test_pca)

# Detections plot
plot_detection("PCA scores", q, ts_ottawa, scores_pca, anoms_pca)

# Plot distributions of anomaly scores
plot_dist("PCA scorer", scores_train_pca, scores_test_pca)

# 3D anomalies plot
plot_anom3d("PCA scorer", ts_ottawa, anoms_pca, px_width, px_height, html = True)

```

#### Principal components plots with T-SNE

```{python PCAVariances}

# Variances explained by each component: First 3 PCs explain almost 99%
pc_variances = scorer_pca.model.explained_variance_ratio_ * 100
pc_variances = [round(x, 2) for x in pc_variances]
pc_variances

```

```{python PCAHeatplot}
#| code-fold: true
#| code-summary: "Show code to generate PC loadings heatplot"

# Heatplot of PC loadings, X = PCs, Y = Features's contribution to the PCs
pc_loadings = pd.DataFrame(
  scorer_pca.model.components_.T,
  columns = pc_variances,
  index = ts_ottawa.components)
_ = sns.heatmap(pc_loadings, cmap = "PiYG")
_ = plt.title("PC loadings")
_ = plt.xlabel("% variances explained by PCs")
_ = plt.ylabel("Features")
plt.show()
plt.close("all")

```

```{python PCA3DPlots}
#| code-fold: true
#| code-summary: "Show code to generate 3D PC plots"

# Transform data into PC values
pcs = scorer_pca.model.detector_.transform(
  scorer_pca.model.scaler_.transform(ts_ottawa.values())
  )

# Principal components plot, first 3 PCs  
fig = px.scatter_3d(
  x = pcs[:, 0],
  y = pcs[:, 1],
  z = pcs[:, 2],
  color = anoms_pca.univariate_values().astype(str),
  title = "PCA components plot, first 3 PCs",
  labels = {
    "x": "PC1",
    "y": "PC2",
    "z": "PC3",
    "color": "Anomaly labels"},
    width = px_width,
    height = px_height
)
fig.show()

# Principal components plot, PC3-4-5
fig = px.scatter_3d(
  x = pcs[:, 4],
  y = pcs[:, 3],
  z = pcs[:, 2],
  color = anoms_pca.univariate_values().astype(str),
  title = "PCA components plot, middle 3 PCs",
  labels = {
    "x": "PC5",
    "y": "PC4",
    "z": "PC3",
    "color": "Anomaly labels"},
    width = px_width,
    height = px_height
)
fig.show()

# Principal components plot, last 3 PCs
fig = px.scatter_3d(
  x = pcs[:, -1],
  y = pcs[:, -2],
  z = pcs[:, -3],
  color = anoms_pca.univariate_values().astype(str),
  title = "PCA components plot, last 3 PCs",
  labels = {
    "x": "PC6",
    "y": "PC7",
    "z": "PC8",
    "color": "Anomaly labels"},
    width = px_width,
    height = px_height
)
fig.show()

```

```{python PCATSNE}
#| code-fold: true
#| code-summary: "Show code to generate 3D T-SNE plot"

# Standardize & normalize PCs
std_scaler = StandardScaler()
pcs_scaled = std_scaler.fit_transform(pcs)

# Apply T-SNE to PCs
tsne = TSNE(n_components = 3)
z_pca = tsne.fit_transform(pcs_scaled)

# T-SNE dimensions plot
fig = px.scatter_3d(
  x = z_pca[:, 0],
  y = z_pca[:, 1],
  z = z_pca[:, 2],
  color = anoms_pca.univariate_values().astype(str),
  title = "PCA components plot, 3D T-SNE reduction",
  labels = {
    "x": "Dim1",
    "y": "Dim2",
    "z": "Dim3",
    "color": "Anomaly labels"},
    width = px_width,
    height = px_height
)
fig.show()

```

### Isolation forest

```{python CreateIForest}

# Create IForest scorer
iforest = IForest(
  n_estimators= 500, # N. of trees in forest
  contamination = (1 - q),
  random_state = 1923)
scorer_iforest = PyODScorer(model = iforest, window = 1)

```

```{python IForestCode}
#| code-fold: true
#| code-summary: "Show code for IForest anomaly detection"

# Perform anomaly scoring
scores_train_iforest, scores_test_iforest, scores_iforest = score(
  ts_train, ts_test, scorer_iforest)

# Perform anomaly detection
anoms_train_iforest, anoms_test_iforest, anoms_iforest = detect(
  scores_train_iforest, scores_test_iforest, detector)

# Plot scores & original series
plot_series(
  "IForest scorer", ts_train, ts_test, scores_train_iforest, scores_test_iforest)

# Detections plot
plot_detection("IForest scores", q, ts_ottawa, scores_iforest, anoms_iforest)

# Plot distributions of anomaly scores
plot_dist("IForest scorer", scores_train_iforest, scores_test_iforest)

# 3D anomalies plot
plot_anom3d(
  "IForest scorer", ts_ottawa, anoms_iforest, px_width, px_height, html = True)

```

```{python IForestFeatImp}
#| code-fold: true
#| code-summary: "Show code to plot IForest feature importances"

# Feature importances (can be misleading for high cardinality features, e.g. day
# and week features)
feat_imp = pd.DataFrame({
  "Feature importance": scorer_iforest.model.feature_importances_,
  "Feature": ts_ottawa.components
}).sort_values("Feature importance", ascending = False)
_ = sns.barplot(data = feat_imp, x = "Feature importance", y = "Feature")
plt.show()
plt.close("all")

```

## Autoencoder with PyTorch Lightning

### Hyperparameter tuning

```{python TuneAutoencoder}
#| code-fold: true
#| code-summary: "Show code to tune Autoencoder hyperparameters"
#| eval: false


# Split train - val data
val_start = pd.Timestamp("1970-01-01")
train_end = pd.Timestamp("1969-12-31")
ts_tr = ts_train.drop_after(val_start)
ts_val = ts_train.drop_before(train_end)

 # Perform preprocessing for train - validation split
x_tr = std_scaler.fit_transform(ts_tr.values())
x_val = std_scaler.transform(ts_val.values())

# Load data into TrainDataset
train_data = TrainDataset(x_tr)
val_data = TrainDataset(x_val)

# Create data loaders
train_loader = torch.utils.data.DataLoader(
      train_data, batch_size = 128, num_workers = 0, shuffle = True)
val_loader = torch.utils.data.DataLoader(
      val_data, batch_size = len(ts_val), num_workers = 0, shuffle = False)


# Define Optuna objective
def objective_nn(trial, train_loader, val_loader):

  # Define parameter ranges to tune over & suggest param set for trial
  hidden_size = trial.suggest_int("hidden_size", 2, 6, step = 2)
  latent_size = trial.suggest_int("latent_size", 1, (hidden_size - 1))
  learning_rate = trial.suggest_float("learning_rate", 5e-4, 5e-2)
  dropout = trial.suggest_float("dropout", 1e-4, 0.2)

  # Create hyperparameters dict
  hyperparams_dict = {
      "input_size": ts_train.values().shape[1],
      "hidden_size": hidden_size,
      "latent_size": latent_size,
      "learning_rate": learning_rate,
      "dropout": dropout}

  # Validate hyperparameter set
  score, epoch = validate_nn(hyperparams_dict, train_loader, val_loader, trial)

  # Report best n. of epochs
  trial.set_user_attr("n_epochs", epoch)

  return score


# Create study
study_nn = optuna.create_study(
  sampler = optuna.samplers.TPESampler(seed = 1923),
  pruner = optuna.pruners.HyperbandPruner(),
  study_name = "tune_nn",
  direction = "minimize"
)

# Instantiate objective
obj = lambda trial: objective_nn(trial, train_loader, val_loader)

# Optimize study
study_nn.optimize(obj, n_trials = 500, show_progress_bar = True)

# Retrieve and export trials
trials_nn = study_nn.trials_dataframe().sort_values("value", ascending = True)
trials_nn.to_csv("./OutputData/trials_nnX.csv", index = False)

```

### Anomaly detection

```{python TrainAutoencoder}
#| code-fold: true
#| code-summary: "Show code to train & predict with Autoencoder model"


# Import best trial
best_trial_nn = pd.read_csv("./OutputData/trials_nn1.csv").iloc[0,]

# Retrieve best hyperparameters
hyperparams_dict = {
      "input_size": ts_train.values().shape[1],
      "hidden_size": best_trial_nn["params_hidden_size"],
      "latent_size": best_trial_nn["params_latent_size"],
      "learning_rate": best_trial_nn["params_learning_rate"],
      "dropout": best_trial_nn["params_dropout"]}


# Perform preprocessing
x_train = std_scaler.fit_transform(ts_train.values())
x_test = std_scaler.transform(ts_test.values())

# Load data into TrainDataset & TestDataset
train_data = TrainDataset(x_train)
test_data = TestDataset(x_test)
train_score_data = TestDataset(x_train)

# Create data loaders
train_loader = torch.utils.data.DataLoader(
      train_data, batch_size = 128, num_workers = 0, shuffle = True)
test_loader = torch.utils.data.DataLoader(
      test_data, batch_size = len(ts_test), num_workers = 0, shuffle = False)
train_score_loader = torch.utils.data.DataLoader(
      train_score_data, batch_size = len(ts_train), num_workers = 0, shuffle = False)


# Create trainer
trainer = L.Trainer(
    max_epochs = int(best_trial_nn["user_attrs_n_epochs"]),
    accelerator = "gpu", devices = "auto", precision = "16-mixed",
    enable_model_summary = True,
    logger = True,
    enable_progress_bar = True,
    enable_checkpointing = True
    )

# Create & train model
model = AutoEncoder(hyperparams_dict = hyperparams_dict)
trainer.fit(model, train_loader)

# Perform reconstructions of training & testing data
preds_train = trainer.predict(
  model, train_score_loader)[0].cpu().numpy().astype(np.float64)
preds_test = trainer.predict(
  model, test_loader)[0].cpu().numpy().astype(np.float64)

```

```{python ScoreAutoencoder}
#| code-fold: true
#| code-summary: "Show code to get reconstruction errors"


# Perform anomaly scoring: Get mean reconstruction error for each datapoint

# Train set
scores_train = np.abs(x_train - preds_train) # Absolute errors of each dimensions
scores_train = np.mean(scores_train, axis = 1) # Mean absolute error over all dimensions
scores_train = pd.DataFrame(scores_train, index = ts_train.time_index) # Dataframe with corresponding dates
scores_train = TimeSeries.from_dataframe(scores_train) # Darts TS
scores_train = scores_train.with_columns_renamed("0", "Scores")

# Test set
scores_test = np.abs(x_test - preds_test)
scores_test = np.mean(scores_test, axis = 1) 
scores_test = pd.DataFrame(scores_test, index = ts_test.time_index)
scores_test = TimeSeries.from_dataframe(scores_test)
scores_test = scores_test.with_columns_renamed("0", "Scores")
scores = scores_train.append(scores_test)

```

```{python CodeAutoencoder}
#| code-fold: true
#| code-summary: "Show code for Autoencoder anomaly detection"

# Perform anomaly detection
anoms_train, anoms_test, anoms = detect(scores_train, scores_test, detector)

# Plot scores & original series
plot_series("Autoencoder scorer", ts_train, ts_test, scores_train, scores_test)

# Detections plot
plot_detection("Autoencoder scores", q, ts_ottawa, scores, anoms)

# Plot distributions of anomaly scores
plot_dist("Autoencoder scorer", scores_train, scores_test)

# 3D anomalies plot
plot_anom3d(
  "Autoencoder scorer", ts_ottawa, anoms, px_width, px_height, html = True)

```

### Latent space plot with T-SNE

```{python AutoencoderTSNE}
#| code-fold: true
#| code-summary: "Show code to generate 3D T-SNE plot"


# Define function to get latent space representations
def get_latent(model, dataloader):
  model.eval() # Put model into inference mode
  with torch.no_grad(): # Disable gradient calculation
    z = [model.forward(x) for _, x in enumerate(dataloader)]
    return z[0].cpu().numpy().astype(np.float64)

# Retrieve & concatenate latent space representations
z_train = get_latent(model, train_score_loader)
z_test = get_latent(model, test_loader)
z = np.concatenate((z_train, z_test), axis = 0)


# Apply T-SNE to latent space
z_scaled = std_scaler.fit_transform(z)
z_autoencoder = tsne.fit_transform(z_scaled)


# Latent space plot
fig = px.scatter_3d(
  x = z_autoencoder[:, 0],
  y = z_autoencoder[:, 1],
  z = z_autoencoder[:, 2],
  color = anoms.univariate_values().astype(str),
  title = "Autoencoder latent space plot, 3D T-SNE reduction",
  labels = {
    "x": "Dim1",
    "y": "Dim2",
    "z": "Dim3",
    "color": "Anomaly labels"},
    width = px_width,
    height = px_height
)
fig.show()

```

## Conclusion
